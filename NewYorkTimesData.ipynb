{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **New York Times Data Collection**"
      ],
      "metadata": {
        "id": "YJR0I4Fw8meH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing New York Times News Category Dataset and Sentiment Analysis"
      ],
      "metadata": {
        "id": "RGBx-Ega1Xzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install textblob"
      ],
      "metadata": {
        "id": "EQqXvthSv-LX",
        "outputId": "99a39079-f5e9-499e-d955-ba64c4cebaa3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.9/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.9/dist-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk>=3.1->textblob) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk>=3.1->textblob) (8.1.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk>=3.1->textblob) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk>=3.1->textblob) (4.65.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas"
      ],
      "metadata": {
        "id": "EILaJ0YJxGkG",
        "outputId": "53fe89ad-e8b5-4f4b-8af8-12392c5fc975",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (1.5.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.9/dist-packages (from pandas) (1.22.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "nltk.download('vader_lexicon')\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "apikey = os.getenv('NYTIMES_APIKEY', 'SqzyHe7mmHI7o9uARoryVwi8wCVHdKzJ')\n",
        "\n",
        "start_date = datetime.strptime('2022-03-13', \"%Y-%m-%d\")\n",
        "end_date = datetime.strptime('2022-09-13', \"%Y-%m-%d\")\n",
        "\n",
        "ny_data = {}\n",
        "all_articles = []\n",
        "\n",
        "# Iterate through months in the date range\n",
        "current_date = start_date\n",
        "while current_date <= end_date:\n",
        "    year = current_date.year\n",
        "    month = current_date.month\n",
        "\n",
        "    query_url = f\"https://api.nytimes.com/svc/archive/v1/{year}/{month}.json?api-key={apikey}\"\n",
        "\n",
        "    r = requests.get(query_url)\n",
        "    ny_data.update(r.json())\n",
        "    all_articles.extend(ny_data['response']['docs'])\n",
        "\n",
        "    # Move to the next month\n",
        "    if current_date.month == 12:\n",
        "        current_date = current_date.replace(year=current_date.year + 1, month=1)\n",
        "    else:\n",
        "        current_date = current_date.replace(month=current_date.month + 1)\n",
        "\n",
        "# Filter articles and calculate sentiment\n",
        "filtered_articles = []\n",
        "for doc in all_articles:\n",
        "    sec_name = doc['section_name']\n",
        "    if sec_name not in ['Business Day', 'Technology', 'Real Estate']:\n",
        "        continue\n",
        "\n",
        "    pub_date = doc['pub_date']\n",
        "    date = datetime.strptime(pub_date.split('T')[0], \"%Y-%m-%d\").date()\n",
        "    if date < start_date.date() or date > end_date.date():\n",
        "        continue\n",
        "\n",
        "    headline = doc['headline']['main']\n",
        "    abstract = doc['abstract']\n",
        "\n",
        "    headline_sentiment = sia.polarity_scores(headline)['compound']\n",
        "    abstract_sentiment = sia.polarity_scores(abstract)['compound']\n",
        "\n",
        "    filtered_articles.append({\n",
        "        'pub_date': str(date),\n",
        "        'headline': headline,\n",
        "        'abstract': abstract,\n",
        "        'section': sec_name,\n",
        "        'headline_sentiment': headline_sentiment,\n",
        "        'abstract_sentiment': abstract_sentiment\n",
        "    })\n",
        "\n",
        "# Create DataFrame\n",
        "cleaned_ny_data_df = pd.DataFrame(filtered_articles)\n",
        "\n",
        "# Group by date and calculate mean and median sentiment scores\n",
        "grouped_data = cleaned_ny_data_df.groupby('pub_date').agg({\n",
        "    'headline_sentiment': ['mean', 'median'],\n",
        "    'abstract_sentiment': ['mean', 'median']\n",
        "}).reset_index()\n",
        "\n",
        "# Flatten the multi-level column names\n",
        "grouped_data.columns = ['_'.join(col).strip() for col in grouped_data.columns.values]\n",
        "\n",
        "# Rename columns\n",
        "grouped_data.columns = ['Date', 'Headline Mean', 'Headline Median', 'Body Mean', 'Body Median']\n",
        "\n",
        "# Set the 'Date' column as the index of the grouped_data DataFrame\n",
        "grouped_data.set_index('Date', inplace=True)\n",
        "\n",
        "# Round the sentiment scores to two decimal places\n",
        "grouped_data_rounded = grouped_data.round(2)\n",
        "\n",
        "# Reindex the DataFrame to include the entire date range\n",
        "start_date = pd.to_datetime('2022-03-13')\n",
        "end_date = pd.to_datetime('2022-09-13')\n",
        "date_range = pd.date_range(start_date, end_date, freq='D')\n",
        "grouped_data_rounded_reindexed = grouped_data_rounded.reindex(date_range)\n",
        "\n",
        "# Save the entire rounded grouped_data DataFrame to a CSV file\n",
        "grouped_data_rounded.to_csv(\"ny_times_data.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFz68TRAtsCA",
        "outputId": "5bbd73e3-cbbb-4094-d4cd-889218bbc456"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter articles and calculate sentiment\n",
        "filtered_articles = []\n",
        "for doc in all_articles:\n",
        "    pub_date = doc['pub_date']\n",
        "    date = datetime.strptime(pub_date.split('T')[0], \"%Y-%m-%d\").date()\n",
        "    if date < start_date.date() or date > end_date.date():\n",
        "        continue\n",
        "\n",
        "    headline = doc['headline']['main']\n",
        "    abstract = doc['abstract']\n",
        "\n",
        "    headline_sentiment = sia.polarity_scores(headline)['compound']\n",
        "    abstract_sentiment = sia.polarity_scores(abstract)['compound']\n",
        "\n",
        "    filtered_articles.append({\n",
        "        'pub_date': str(date),\n",
        "        'headline': headline,\n",
        "        'abstract': abstract,\n",
        "        'section': doc['section_name'],\n",
        "        'headline_sentiment': headline_sentiment,\n",
        "        'abstract_sentiment': abstract_sentiment\n",
        "    })\n",
        "\n",
        "# Create DataFrame\n",
        "cleaned_ny_data_df = pd.DataFrame(filtered_articles)\n",
        "\n",
        "# Group by date and calculate max and min sentiment scores\n",
        "grouped_data = cleaned_ny_data_df.groupby('pub_date').agg({\n",
        "    'headline_sentiment': ['max', 'min'],\n",
        "    'abstract_sentiment': ['max', 'min']\n",
        "}).reset_index()\n",
        "\n",
        "# Flatten the multi-level column names\n",
        "grouped_data.columns = ['_'.join(col).strip() for col in grouped_data.columns.values]\n",
        "\n",
        "# Rename columns\n",
        "grouped_data.columns = ['Date', 'Headline Max', 'Headline Min', 'Body Max', 'Body Min']\n",
        "\n",
        "# Set the 'Date' column as the index of the grouped_data DataFrame\n",
        "grouped_data.set_index('Date', inplace=True)\n",
        "\n",
        "# Round the sentiment scores to two decimal places\n",
        "grouped_data_rounded = grouped_data.round(2)\n",
        "\n",
        "# Save the entire rounded grouped_data DataFrame to a CSV file\n",
        "grouped_data_rounded.to_csv(\"ny_times_data_max_min.csv\")"
      ],
      "metadata": {
        "id": "U2lR7apJAqnr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in the two CSV files\n",
        "df1 = pd.read_csv('ny_times_data.csv')\n",
        "df2 = pd.read_csv('ny_times_data_max_min.csv')\n",
        "\n",
        "# Merge the two dataframes using the date column as the key\n",
        "merged_df = pd.merge(df1, df2, on='Date')\n",
        "\n",
        "# Reorder the columns so that the date column is first\n",
        "merged_df = merged_df[['Date', 'Headline Mean', 'Headline Median', 'Body Mean', 'Body Median', 'Headline Max', 'Headline Min', 'Body Max', 'Body Min']]\n",
        "\n",
        "# Write the merged dataframe to a new CSV file\n",
        "merged_df.to_csv('ny_times_merged_data.csv', index=False)\n"
      ],
      "metadata": {
        "id": "BXe9_m08H8CN"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}